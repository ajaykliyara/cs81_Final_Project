---
title: "Untitled"
author: "Will Williamson"
date: "November 28, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load libraries
library(stringr)
library(plyr)
library(lubridate)
library(randomForest)
library(reshape2)
library(ggplot2)
library(zoo)
library(dplyr)
library(ROCR)
library(pROC)
library(h2o)
library(caret)
library(readr)
```

```{r}
### Automatic feature selection with caret

# ensure the results are repeatable
set.seed(7)

# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)

# run the RFE algorithm
df_no_na <- na.omit(df.term)
results <- rfe(df_no_na[,1:6], df_no_na[,7], sizes=c(1:7), rfeControl=control)

# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
```


```{r}
### Large data frame for feature selection
# read the csv data file
df <- read.csv("data/LoanStats3a_securev1.csv", header = TRUE,
                stringsAsFactors = F)

#get rid of fields that are 80% NA
poor_coverage <- sapply(df, function(x) {
  coverage <- 1 - sum(is.na(x)) / length(x)
  coverage < 0.8
})
df <- df[,poor_coverage==FALSE]

# delete columns that are obviously not going to help
col_to_delete <- c("id", "member_id", "funded_amnt", "funded_amnt_inv", "int_rate",
                   "installment", "emp_title", "verification_status", "issue_d",
                   "pymnt_plan", "url", "desc", "purpose", "title",
                   "initial_list_status", "out_prncp", "out_prncp_inv",
                   "total_pymnt", "total_pymnt_inv",
                   "total_rec_prncp", "total_rec_int", "total_rec_late_fee",
                   "recoveries", "collection_recovery_fee", "last_pymnt_d",
                   "last_pymnt_amnt", "next_pymnt_d", "last_credit_pull_d",
                   "application_type", "grade", "sub_grade", "policy_code")
df <- df %>% select(-one_of(col_to_delete))

# clean up emp_length column
df$emp_length <- str_match(df$emp_length, "[0123456789+]")
df$emp_length <- as.numeric(df$emp_length)

# remove '%' from revol_util
df$revol_util <- gsub("%", "", df$revol_util)
df$revol_util <- as.numeric(df$revol_util)

# Remove all remaining rows with NA's
df <- na.omit(df)

# remove columns which are all zeros
all_zeros <- sapply(df, function(x) {
  col_sum = 1
  if (is.numeric(x)){
      col_sum = sum(x)
  }
  col_sum == 0
})
df <- df[,all_zeros==FALSE]

# remove "months" from the term column
df$term <- str_match(df$term, "[0123456789+]")
df$term <- as.numeric(df$term)

# create a good / bad column
# bad if late more than 30 days, in default, or charged off
bad_indicators <- c("Late (16-30 days)", "Late (31-120 days)", "Default", "Charged Off")
df <- df %>%
  mutate(is_bad = ifelse(df$loan_status %in% bad_indicators, 1, 0)) %>%
  select(-loan_status)

# convert earliest credit line dates to time since epoch
df$earliest_cr_line <- as.numeric(as.Date(as.yearmon(df$earliest_cr_line, "%b-%y")))

write.csv(df, "data/loanstats_2015_large.csv")
```

```{r}
## get_splits
get_splits <- function(data, factor_cols, class_col_name = "is_bad"){
  data[,factor_cols] <- as.factor(data[,factor_cols])
  
  # split into train, validation, and test sets
  splits <- h2o.splitFrame(data = data,
                           ratios = c(0.7, 0.15),  #partition data into 70%, 15%, 15% chunks
                           seed = 1)  #setting a seed will guarantee reproducibility
  train <- splits[[1]]
  valid <- splits[[2]]
  test <- splits[[3]]
  
  # Create lists which indicate independent and dependent variable names in the data
  y <- class_col_name
  x <- setdiff(names(data), class_col_name)
  
  # package up the return values
  split_list <- list()
  split_list$train <- train
  split_list$valid <- valid
  split_list$test <- test
  split_list$x <- x
  split_list$y <- y
  return(split_list)
}

```

```{r}
## get_best_model
get_best_model <- function(split_list, grid_id){
  # extract the best model from the grid
  gridperf <- h2o.getGrid(grid_id = grid_id,
                          sort_by = "auc",
                          decreasing = TRUE)
  
  best_model <- h2o.getModel(gridperf@model_ids[[1]])
  
  # make a prediction using the best model and the test data
  perf <- h2o.performance(model = best_model,
                          newdata = split_list$test)
  
  cat(grid_id, " AUC: ", h2o.auc(perf))
  
  return(best_model)
}
```

```{r}
#### Random Forest Model
get_rf_grid <- function(data, factor_cols, max_num_models = 25, 
                        class_col_name = "is_bad", in_grid_id = "rf_grid"){
  data[,factor_cols] <- as.factor(data[,factor_cols])
  
  # split into train, validation, and test sets
  split_list <- get_splits(data, factor_cols)
  
  # define a random forest search grid
  random_search_criteria <- list(strategy = "RandomDiscrete", max_models = max_num_models)
  rf_hyper_params <- list(ntrees = c(100,200,300,400,500,600,700,800,900,1000), 
                          mtries = c(2:(ncol(data)-1)), max_depth = c(2:20)) 
  
  # fit the random forest search grid
  h2o.rf_grid <- h2o.grid("randomForest", x = split_list$x, y = split_list$y, 
                          training_frame = split_list$train,
                          validation_frame = split_list$valid,
                          hyper_params = rf_hyper_params,
                          search_criteria = random_search_criteria,
                          seed = 3, 
                          grid_id = in_grid_id)
  
  best_model <- get_best_model(split_list, in_grid_id)
  
  return(best_model)
}
```

```{r}
## Feature Selection with H2o
h2o.init(nthreads = -1, #Number of threads -1 means use all cores on your machine
        max_mem_size = "16G")  #max mem size is the maximum memory to allocate to H2O

# read the data set
df1 <- read_csv("data/loanstats_2015_large.csv")
df1 <- df1 %>% select(-X1)
data <- as.h2o(df1)

# encode the factors as factors
factor_cols <- c("is_bad", "home_ownership", "zip_code", "addr_state")

# compute the random forest model
best_model <- get_rf_grid(data, factor_cols, 2)

# predictor importance
h2o.varimp(best_model)
h2o.varimp_plot(best_model)

# remove the last grid
h2o.rm("rf_grid")

# reduce the number of features based on the previous model results.  
# try averaging the last fico's and removing the fico high and low
df1 <- df1 %>% 
  mutate(fico = (last_fico_range_high + last_fico_range_low) / 2) %>%
  select(-c(fico_range_high, fico_range_low, last_fico_range_high, 
            last_fico_range_low))

# create a new h2o data frame
data <- as.h2o(df1)

# compute a new random forest model
best_model <- get_rf_grid(data, factor_cols, 2)

# print predictor importance and make a plot
h2o.varimp(best_model)
h2o.varimp_plot(best_model)

# select the best 12 predictors
best_predictors <- h2o.varimp(best_model)$variable
best_predictors <- best_predictors[1:12]
best_predictors <- append(best_predictors, c("is_bad"))
bar <- select_(df1, .dots = best_predictors)

# re-encode the factors removing home_ownership
factor_cols <- c("is_bad", "zip_code", "addr_state")

# remove the last grid 
h2o.rm("rf_grid")

# compute a new model with the best predictors
best_model <- get_rf_grid(data, factor_cols, 2)

# remove the last grid
h2o.rm("rf_grid")

write_csv(df1, "data/loanstats_2015_optimized.csv")
```

```{r}
#### GBM Model
get_gbm_grid <- function(data, factor_cols, max_num_models = 25, 
                         class_col_name = "is_bad", in_grid_id = "gbm_grid"){
  data[,factor_cols] <- as.factor(data[,factor_cols])
  
  # split into train, validation, and test sets
  split_list <- get_splits(data, factor_cols)
  
  # define random search grid
  random_search_criteria <- list(strategy = "RandomDiscrete", max_models = max_num_models)
  
  # GBM hyperparamters
  gbm_hyper_params <- list(learn_rate = c(0.01, 0.1),
                      max_depth = c(3, 5, 9),
                      sample_rate = c(0.8, 1.0),
                      col_sample_rate = c(0.2, 0.5, 1.0))
  
  # Train and validate a grid of GBMs
  gbm_grid <- h2o.grid("gbm", x = split_list$x, y = split_list$y,
                       grid_id = in_grid_id,
                       training_frame = split_list$train,
                       validation_frame = split_list$valid,
                       ntrees = 100,
                       seed = 3,
                       hyper_params = gbm_hyper_params,
                       search_criteria = random_search_criteria)
  
  best_model <- get_best_model(split_list, in_grid_id)
  
  return(best_model)
}

h2o.rm("gbm_grid")

# sample gbm call
# gbm_model <- get_gbm_grid(data, factor_cols, 2)
```


```{r}
#### GLM Model
get_glm <- function(data, factor_cols, 
                    class_col_name = "is_bad", 
                    in_model_id = "glm"){
  data[,factor_cols] <- as.factor(data[,factor_cols])
  
  # split into train, validation, and test sets
  split_list <- get_splits(data, factor_cols)
  
  # Train the GLM Model
  glm_fit <- h2o.glm(x = split_list$x, 
                     y = split_list$y, 
                     training_frame = split_list$train,
                     model_id = in_model_id,
                     validation_frame = split_list$valid,
                     family = "binomial",
                     lambda_search = TRUE)
  
  glm_perf <- h2o.performance(model = glm_fit,
                              newdata = split_list$test)
  cat(in_model_id, " AUC: ", h2o.auc(glm_perf))
  
  return(glm_fit)
}

# h2o.rm("glm")

# sample gbm call
# glm_model <- get_glm(data, factor_cols)
```

```{r}
#### Naive Bayes Model
get_nb <- function(data, factor_cols, 
                   class_col_name = "is_bad", 
                   in_model_id = "naive_bayes"){
  data[,factor_cols] <- as.factor(data[,factor_cols])
  
  # split into train, validation, and test sets
  split_list <- get_splits(data, factor_cols)
  
  # Train the NB model
  nb_fit <- h2o.naiveBayes(x = split_list$x,
                           y = split_list$y,
                           training_frame = split_list$train,
                           model_id = in_model_id)
  
  nb_perf <- h2o.performance(model = nb_fit,
                              newdata = split_list$test)
  cat(in_model_id, " AUC: ", h2o.auc(nb_perf))
  
  return(nb_fit)
}

# h2o.rm("naive_bayes")

# sample naive bayes call
# nb_model <- get_nb(data, factor_cols)
```











