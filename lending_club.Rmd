---
title: "Untitled"
author: "Will Williamson"
date: "November 28, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load libraries
library(stringr)
library(plyr)
library(lubridate)
library(randomForest)
library(reshape2)
library(ggplot2)
library(zoo)
library(dplyr)
library(ROCR)
library(pROC)
library(h2o)
library(caret)
library(readr)
library(h2oEnsemble)
library(cvAUC)
library(syuzhet)
```

```{r}
## Start the H2o engine
h2o.init(nthreads = -1, #Number of threads -1 means use all cores on your machine
         max_mem_size = "16G")  #max mem size is the maximum memory to allocate to H2O
```


```{r}
### Large data frame for feature selection
# read the csv data file
df <- read.csv("data/LoanStats3a_securev1.csv", header = TRUE,
                stringsAsFactors = F)

orig_col<- c("loan_amnt", "purpose", "grade", "sub_grade", "int_rate", "term", "installment", "home_ownership",
             "title", "emp_length", "annual_inc", "dti", "zip_code", "addr_state", "fico_range_low", 
             "fico_range_high", "earliest_cr_line", "open_acc", "total_acc", "revol_bal", "revol_util",
             "inq_last_6mths", "acc_now_delinq", "delinq_amnt", "delinq_2yrs", "mths_since_last_delinq", 
             "pub_rec", "mths_since_last_record",  "mths_since_last_major_derog", "collections_12_mths_ex_med",
             "loan_status", "desc")
df <- df %>% select(one_of(orig_col))

# columns to delete
col_to_delete <- c("purpose", "title", "collections_12_mths_ex_med", "mths_since_last_major_derog")
df <- df %>% select(-one_of(col_to_delete))

# clean up emp_length column
df$emp_length <- str_match(df$emp_length, "[0123456789+]")
df$emp_length <- as.numeric(df$emp_length)

# remove '%' from revol_util and int_rate
df$revol_util <- gsub("%", "", df$revol_util)
df$revol_util <- as.numeric(df$revol_util)
df$int_rate <- gsub("%", "", df$int_rate)
df$int_rate <- as.numeric(df$int_rate)

# clean up NA's
# what columns have na values??
na_col_list <- list()
i <- 1
for (col in names(df)){
  num_na <- sum(is.na(df[, col]))
  if (num_na > 0){
    cat(col, ": ", num_na, "\n")
    na_col_list[i] <- col
    i <- i + 1
  }
}

# mths_since_last_delinq, emp_length
df <- df %>%
  mutate(mths_since_last_delinq = ifelse(is.na(mths_since_last_delinq), 0, mths_since_last_delinq)) %>%
  mutate(emp_length = ifelse(is.na(emp_length), 0, emp_length)) %>%
  mutate(open_acc = ifelse(is.na(open_acc), 0, open_acc)) %>%
  mutate(mths_since_last_record = ifelse(is.na(mths_since_last_record), 0, mths_since_last_record))

# Remove all remaining rows with NA's
df <- na.omit(df)

# remove columns which are all zeros
all_zeros <- sapply(df, function(x) {
  col_sum = 1
  if (is.numeric(x)){
      col_sum = sum(x)
  }
  col_sum == 0
})
df <- df[,all_zeros==FALSE]

# remove "months" from the term column
df$term <- str_match(df$term, "[0123456789+]")
df$term <- as.numeric(df$term)

# add description sentiment column
df <- df %>%
  mutate(sentiment = as.numeric(get_sentiment(desc))) %>%
  select(-one_of("desc"))

# create a good / bad column
# bad if late more than 30 days, in default, or charged off
bad_indicators <- c("Default", "Charged Off", "Late (31-120 days)", "Does not meet the credit policy. Status:Charged Off")
df <- df %>%
  mutate(is_bad = ifelse(df$loan_status %in% bad_indicators, 1, 0)) %>%
  select(-loan_status)

# convert earliest credit line dates to time since epoch
df$earliest_cr_line <- as.numeric(as.Date(as.yearmon(df$earliest_cr_line, "%b-%y")))

write.csv(df, "data/loanstats_2015_large.csv")
```

```{r}
## get_split_list
get_split_list <- function(data, factor_cols, class_col_name = "is_bad"){
  data[,factor_cols] <- as.factor(data[,factor_cols])
  
  # split into train, validation, and test sets
  split_list <- h2o.splitFrame(data = data,
                               ratios = c(0.7, 0.15),  #partition data into 70%, 15%, 15% chunks
                               seed = 1)  #setting a seed will guarantee reproducibility
  train <- split_list[[1]]
  valid <- split_list[[2]]
  test <- split_list[[3]]
  
  # Create lists which indicate independent and dependent variable names in the data
  y <- class_col_name
  x <- setdiff(names(data), class_col_name)
  
  # package up the return values
  split_list <- list()
  split_list$train <- train
  split_list$valid <- valid
  split_list$test <- test
  split_list$x <- x
  split_list$y <- y
  return(split_list)
}

```

```{r}
## get_test_perf
get_test_perf <- function(split_list, model, model_name, print_roc = TRUE){
  # make a prediction using the model and the test data
  test_perf <- h2o.performance(model = model,
                               newdata = split_list$test)
  
  # if the print flag is set
  if (print_roc == TRUE){
    # print AUC and ROC Curve
    cat(model_name, " AUC: ", h2o.auc(perf))
    plot(test_perf)    
  }

  # return an h2o performance object to the caller
  return(perf)
}
```

```{r}
## get_best_model
get_best_model <- function(grid_id){
  # extract the best model from the grid
  gridperf <- h2o.getGrid(grid_id = grid_id,
                          sort_by = "auc",
                          decreasing = TRUE)
  
  best_model <- h2o.getModel(gridperf@model_ids[[1]])
  
  return(best_model)
}
```

```{r}
#### Random Forest Model
get_rf_grid <- function(split_list, max_num_models = 25, 
                        in_grid_id = "rf_grid", 
                        in_balance_classes = FALSE){

  
  # define a random forest search grid
  random_search_criteria <- list(strategy = "RandomDiscrete", max_models = max_num_models)
  rf_hyper_params <- list(ntrees = c(100,200,300,400,500,600,700,800,900,1000), 
                          mtries = c(2:(length(split_list$x)-1)), max_depth = c(2:20),
                          balance_classes = in_balance_classes) 
  
  # fit the random forest search grid
  h2o.rf_grid <- h2o.grid("randomForest", x = split_list$x, y = split_list$y, 
                          training_frame = split_list$train,
                          validation_frame = split_list$valid,
                          hyper_params = rf_hyper_params,
                          search_criteria = random_search_criteria,
                          seed = 3, 
                          grid_id = in_grid_id)
  
  best_model <- get_best_model(in_grid_id)
  
  return(best_model)
}
```

```{r}
## Feature Selection with H2o
# read the data set
df1 <- read_csv("data/loanstats_2015_large.csv")
df1 <- df1 %>% select(-X1)

# encode the factors as factors
factor_cols <- c("is_bad", "home_ownership", "zip_code", "addr_state", "grade", "sub_grade")

# create an h2o data frame
data <- as.h2o(df1)

# compute the random forest model
h2o.rm("rf_grid")
split_list <- get_split_list(data, factor_cols)
rf_model <- get_rf_grid(split_list, 4)
perf <- get_test_perf(data, rf_model, "Random Forest")

# predictor importance
h2o.varimp(best_model)
h2o.varimp_plot(best_model)

# select the best 16 predictors (home ownership no longer included)
#best_predictors <- h2o.varimp(best_model)$variable
#best_predictors <- best_predictors[1:11]
#best_predictors <- append(best_predictors, c("is_bad"))
#df1 <- select_(df1, .dots = best_predictors)

# save the optimized data
write_csv(df1, "data/loanstats_2015_optimized.csv")

# remove the last grid
h2o.rm("rf_grid")
```

```{r}
## Read in the optimized data set
# save the optimized data
df1 <- read_csv("data/loanstats_2015_optimized.csv")
data <- as.h2o(df1)

# re-encode the factors removing home_ownership
factor_cols <- c("is_bad", "zip_code", "addr_state", "grade", "sub_grade", "home_ownership")

# get the data split list
split_list <- get_split_list(data, factor_cols)

# set the number of models to run
num_models <- 4
```


```{r}
## Random Forest Model
h2o.rm("rf_model")

rf_model <- get_rf_grid(split_list, num_models)
perf_rf <- get_test_perf(split_list, rf_model, "Random Forest")
```

```{r}
#### GBM Model
get_gbm_grid <- function(split_list, 
                         max_num_models = 25, 
                         in_grid_id = "gbm_grid"){
  
  # define random search grid
  random_search_criteria <- list(strategy = "RandomDiscrete", max_models = max_num_models)
  
  # GBM hyperparamters
  gbm_hyper_params <- list(learn_rate = c(0.01, 0.1),
                           max_depth = c(seq(3, 15, 2)),
                           sample_rate = c(seq(.5, 1, .1)),
                           col_sample_rate = c(seq(.2, 1, .1)),
                           ntrees = c(seq(100, 500, 50)))
  
  # Train and validate a grid of GBMs
  gbm_grid <- h2o.grid("gbm", x = split_list$x, y = split_list$y,
                       grid_id = in_grid_id,
                       training_frame = split_list$train,
                       validation_frame = split_list$valid,
                       seed = 3,
                       hyper_params = gbm_hyper_params,
                       search_criteria = random_search_criteria)
  
  best_model <- get_best_model(in_grid_id)
  
  return(best_model)
}

h2o.rm("gbm_grid")

# build the gbm model
gbm_model <- get_gbm_grid(split_list,  num_models)
perf_gbm <- get_test_perf(split_list, gbm_model, "GBM")
```


```{r}
#### GLM Model
get_glm <- function(split_list,
                    in_model_id = "glm"){
  
  # Train the GLM Model
  glm_fit <- h2o.glm(x = split_list$x, 
                     y = split_list$y, 
                     training_frame = split_list$train,
                     model_id = in_model_id,
                     validation_frame = split_list$valid,
                     family = "binomial",
                     lambda_search = TRUE)
  
  glm_perf <- h2o.performance(model = glm_fit,
                              newdata = split_list$test)
  
  return(glm_fit)
}

h2o.rm("glm")

# build the gbm model
glm_model <- get_glm(split_list)
perf_glm <- get_test_perf(split_list, glm_model, "GLM")
```

```{r}
#### Naive Bayes Model
get_nb <- function(split_list, 
                   in_model_id = "naive_bayes"){
  
  # Train the NB model
  nb_fit <- h2o.naiveBayes(x = split_list$x,
                           y = split_list$y,
                           training_frame = split_list$train,
                           model_id = in_model_id)
  
  nb_perf <- h2o.performance(model = nb_fit,
                             newdata = split_list$test)
  
  return(nb_fit)
}

h2o.rm("naive_bayes")

# build the naive bayes model
nb_model <- get_nb(split_list)
perf_nb <- get_test_perf(split_list, nb_model, "Naive Bayes")
```

```{r}
#### Deep Learning Model
get_dl_grid <- function(data, factor_cols, in_max_run_time_sec = 0, 
                        max_epochs = 1000000, class_col_name = "is_bad", 
                        in_grid_id = "dl_grid", in_max_runtime_secs = 0){
  data[,factor_cols] <- as.factor(data[,factor_cols])
  
  # split into train, validation, and test sets
  split_list <- get_split_list(data, factor_cols)
  
  # Deeplearning hyperparamters
  activation_opt <- c("Rectifier", "Tanh", "TanhWithDropout", "RectifierWithDropout", 
                      "Maxout", "MaxoutWithDropout")
  l1_opt <- c(0, 0.00001, 0.0001, 0.001, 0.01, 0.1)
  l2_opt <- c(0, 0.00001, 0.0001, 0.001, 0.01, 0.1)
  hidden_layers = list(c(8,8), c(10,10),c(12,12), c(14,14), c(8,8,8),
                       c(10,10,10), c(12,12,12), c(14,14,14))
  hyper_params <- list(activation = activation_opt,
                       l1 = l1_opt,
                       l2 = l2_opt,
                       hidden = hidden_layers)
  search_criteria <- list(strategy = "RandomDiscrete", 
                          max_runtime_secs = in_max_run_time_sec)
  
  # Train and validate the deep learning grid
  dl_grid <- h2o.grid("deeplearning", x = split_list$x, y = split_list$y,
                      grid_id = "dl_grid",
                      training_frame = split_list$train,
                      validation_frame = split_list$valid,
                      seed = 3,
                      # hidden = c(10, 10),
                      hyper_params = hyper_params,
                      search_criteria = search_criteria,
                      epochs = max_epochs,
                      stopping_rounds = 2,
                      stopping_metric = "AUC",
                      stopping_tolerance = 0.01,
                      max_runtime_secs = in_max_runtime_secs,
                      quiet_mode = FALSE)
  
  best_model <- get_best_model(split_list, in_grid_id) 
  
  return(dl_grid)
}

h2o.rm("dl_grid")

# build the deep learning model
eight_hours <- 60 * 60 * 8
one_hour <- 60 * 60
dl_grid <- get_dl_grid(data, factor_cols, 
                       in_max_run_time_sec = one_hour,
                       max_epochs = 1000)
```


```{r}
# split into train, validation, and test sets
split_list <- get_split_list(data, factor_cols)

### ensemble stacking
learner <- c("h2o.glm.wrapper", "h2o.randomForest.wrapper", 
             "h2o.gbm.wrapper", "h2o.deeplearning.wrapper")
metalearner <- "h2o.glm.wrapper"

#### Train an Ensemble
#Train the ensemble (using 5-fold internal CV) to generate the level-one data.  Note that more CV folds will take longer to train, but should increase performance.
fit <- h2o.ensemble(x = split_list$x, y = split_list$y, 
                    training_frame = split_list$train, 
                    family = "binomial", 
                    learner = learner, 
                    metalearner = metalearner,
                    cvControl = list(V = 5))

#### Predict 
#Generate predictions on the test set.
pred <- predict(fit, split_list$test)
predictions <- as.data.frame(pred$pred)[,3]  #third column is P(Y==1)
labels <- as.data.frame(split_list$test[,split_list$y])[,1]

# AUC
cvAUC::AUC(predictions = predictions, labels = labels)
```










